{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81bc0f42",
   "metadata": {},
   "source": [
    "Total Shipment Weight\n",
    "\n",
    "Last Updated: July 2025\n",
    "Easy\n",
    "ID 2058\n",
    "76\n",
    "\n",
    "Amazon\n",
    "\n",
    "Calculate the total weight for each shipment and add it as a new column. Your output needs to have all the existing rows and columns in addition to the  new column that shows the total weight for each shipment. One shipment can have multiple rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd84fde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/17 01:07:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Shipment\").getOrCreate()\n",
    "\n",
    "schema = StructType( [   StructField(\"shipmet\",IntegerType(), True) ,\n",
    "                       StructField(\"sub_id\", IntegerType(), True), \n",
    "                       StructField(\"weight\", IntegerType(), True), \n",
    "                        StructField(\"shipment_date\", StringType(), True)  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a194d77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (101, 1, 10, \"2021-08-30\"),\n",
    "    (101, 2, 20, \"2021-09-01\"),\n",
    "    (101, 3, 10, \"2021-09-05\"),\n",
    "    (102, 1, 50, \"2021-09-02\"),\n",
    "    (103, 1, 25, \"2021-09-01\"),\n",
    "    (103, 2, 30, \"2021-09-02\"),\n",
    "    (104, 1, 30, \"2021-08-25\"),\n",
    "    (104, 2, 10, \"2021-08-26\"),\n",
    "    (105, 1, 20, \"2021-09-02\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ef95430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+-------------+\n",
      "|shipmet|sub_id|weight|shipment_date|\n",
      "+-------+------+------+-------------+\n",
      "|    101|     1|    10|   2021-08-30|\n",
      "|    101|     2|    20|   2021-09-01|\n",
      "|    101|     3|    10|   2021-09-05|\n",
      "|    102|     1|    50|   2021-09-02|\n",
      "|    103|     1|    25|   2021-09-01|\n",
      "|    103|     2|    30|   2021-09-02|\n",
      "|    104|     1|    30|   2021-08-25|\n",
      "|    104|     2|    10|   2021-08-26|\n",
      "|    105|     1|    20|   2021-09-02|\n",
      "+-------+------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0fb2914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- shipmet: integer (nullable = true)\n",
      " |-- sub_id: integer (nullable = true)\n",
      " |-- weight: integer (nullable = true)\n",
      " |-- shipment_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739b893d",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2eeae9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+-------------+------------+\n",
      "|shipmet|sub_id|weight|shipment_date|total_weight|\n",
      "+-------+------+------+-------------+------------+\n",
      "|    101|     1|    10|   2021-08-30|          40|\n",
      "|    101|     2|    20|   2021-09-01|          40|\n",
      "|    101|     3|    10|   2021-09-05|          40|\n",
      "|    102|     1|    50|   2021-09-02|          50|\n",
      "|    103|     1|    25|   2021-09-01|          55|\n",
      "|    103|     2|    30|   2021-09-02|          55|\n",
      "|    104|     1|    30|   2021-08-25|          40|\n",
      "|    104|     2|    10|   2021-08-26|          40|\n",
      "|    105|     1|    20|   2021-09-02|          20|\n",
      "+-------+------+------+-------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F \n",
    "\n",
    "# group by the shipment column name in your df\n",
    "df_total = df.groupBy(\"shipmet\") \\\n",
    "             .agg(F.sum(\"weight\").alias(\"total_weight\"))\n",
    "\n",
    "# join back to original df\n",
    "df1 = df.join(df_total, on=\"shipmet\", how=\"left\") \\\n",
    "        .orderBy(\"shipmet\", \"sub_id\", \"weight\", \"shipment_date\")\n",
    "\n",
    "df1.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53576cfd",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4da2eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+-------------+-------------+\n",
      "|shipmet|sub_id|weight|shipment_date|total_weitght|\n",
      "+-------+------+------+-------------+-------------+\n",
      "|    101|     1|    10|   2021-08-30|           40|\n",
      "|    101|     2|    20|   2021-09-01|           40|\n",
      "|    101|     3|    10|   2021-09-05|           40|\n",
      "|    102|     1|    50|   2021-09-02|           50|\n",
      "|    103|     1|    25|   2021-09-01|           55|\n",
      "|    103|     2|    30|   2021-09-02|           55|\n",
      "|    104|     1|    30|   2021-08-25|           40|\n",
      "|    104|     2|    10|   2021-08-26|           40|\n",
      "|    105|     1|    20|   2021-09-02|           20|\n",
      "+-------+------+------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import functions as F \n",
    "from pyspark.sql.window import Window \n",
    "\n",
    "# Define window partitioned by shipment_id\n",
    "window_spec = Window.partitionBy(df.shipmet)\n",
    "\n",
    "# Add total weight  per shipment \n",
    "\n",
    "df1 = df.withColumn(\"total_weitght\", F.sum(df.weight).over(window_spec) ).orderBy(df.shipmet,df.sub_id,df.weight,df.shipment_date)\n",
    "df1.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
